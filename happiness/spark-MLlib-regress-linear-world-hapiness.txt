
val rdd1 = sc.textFile("world_happiness/world_happiness_report_2015.csv").map( x => x.split(","))

val rdd = rdd1.map( x => x.slice(3,x.size).map( x => x.toDouble))

rdd.first
res0: Array[Double] = Array(7.587, 0.03411, 1.39651, 1.34951, 0.94143, 0.66557, 0.41978, 0.29678, 2.51738)

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = rdd.map( x => {
   val arr_size = x.size
   val l = x(0)
   val f = x.slice(1,arr_size)
   LabeledPoint(l,Vectors.dense(f))
 })
 
val sets = data.randomSplit(Array(0.7,0.3), 11L)
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache

---- MLlib logistic regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.regression.LinearRegressionWithSGD
import org.apache.spark.mllib.evaluation.RegressionMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step  -> RMSE, MSE") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LinearRegressionWithSGD
	model.setIntercept(true)
	model.optimizer.setNumIterations(numIter).setStepSize(step)
	val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new RegressionMetrics(validPredicts)
    println("%d, %7.5f -> %.4f, %.4f".format(numIter, step, metrics.rootMeanSquaredError, metrics.meanSquaredError))
  }
}

iterateLRwSGD(Array(100,200,300),Array(1, 0.1, 0.01, 0.001), trainSet, testSet)
iter, step  -> RMSE, MSE
100, 1.00000 -> 0.2240, 0.0502
100, 0.10000 -> 0.4691, 0.2200
100, 0.01000 -> 0.9852, 0.9706
100, 0.00100 -> 3.7611, 14.1458
200, 1.00000 -> 0.2055, 0.0422  *
200, 0.10000 -> 0.4691, 0.2200
200, 0.01000 -> 0.8028, 0.6445
200, 0.00100 -> 3.7519, 14.0766
300, 1.00000 -> 0.2055, 0.0422
300, 0.10000 -> 0.4691, 0.2200
300, 0.01000 -> 0.8028, 0.6445
300, 0.00100 -> 3.7519, 14.0766

----- Decide to scale features because variabiliaty increases even reducing step size of model 

import org.apache.spark.mllib.feature.StandardScaler
val scaler = new StandardScaler(true, true).fit(trainSet.map(x => x.features))
 
val trainScaled = trainSet.map(x => LabeledPoint(x.label,scaler.transform(x.features)))
val testScaled = testSet.map(x => LabeledPoint(x.label, scaler.transform(x.features)))

trainScaled.cache

---- MLlib Linear regression --------------

import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.regression.LinearRegressionWithSGD
import org.apache.spark.mllib.evaluation.RegressionMetrics

def iterateLRwSGD(iterNums:Array[Int], stepSizes:Array[Double], train:RDD[LabeledPoint], test:RDD[LabeledPoint]) = {
  println("iter, step  -> RMSE, MSE") 
  for(numIter <- iterNums; step <- stepSizes) {
    val model = new LinearRegressionWithSGD
	model.setIntercept(true)
	model.optimizer.setNumIterations(numIter).setStepSize(step)
	val lr = model.run(train)
    val validPredicts = test.map(x => (lr.predict(x.features),x.label))
    val metrics = new RegressionMetrics(validPredicts)
    println("%d, %7.5f -> %.4f, %.4f".format(numIter, step, metrics.rootMeanSquaredError, metrics.meanSquaredError))
  }
}

iterateLRwSGD(Array(100,200,300),Array(1, 0.1, 0.01, 0.001), trainScaled, testScaled)
iter, step  -> RMSE, MSE
100, 1.00000 -> 0.0357, 0.0013  *
100, 0.10000 -> 0.6960, 0.4845
100, 0.01000 -> 3.6676, 13.4516
100, 0.00100 -> 4.3511, 18.9323
200, 1.00000 -> 0.0357, 0.0013
200, 0.10000 -> 0.5564, 0.3096
200, 0.01000 -> 3.3887, 11.4831
200, 0.00100 -> 4.3511, 18.9323
300, 1.00000 -> 0.0357, 0.0013
300, 0.10000 -> 0.5564, 0.3096
300, 0.01000 -> 3.2942, 10.8517
300, 0.00100 -> 4.3511, 18.9323

---- MLlib decision tree regression --------------

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int]()  // no categorical features in dataset
val impurity = "variance"
val maxDepth = 5
val maxBins = 32

val model = DecisionTree.trainRegressor(trainSet, categoricalFeaturesInfo, impurity, maxDepth, maxBins)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res8: Array[(Double, Double)] = Array((7.587,7.527), (7.416499999999999,7.522), (6.611000000000001,7.284), (6.635249999999999,7.278), (6.914666666666668,7.226), (7.371,7.2), (7.371,6.853), (6.914666666666668,6.81), (5.985500000000002,6.798), (6.914666666666668,6.455))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  // 0.5620248213460143
validMetrics.meanSquaredError  // 0.3158718998090193
